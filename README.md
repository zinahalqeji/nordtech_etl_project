# ğŸ§° Nordtech ETL Pipeline â€” Developer Guide

A complete, modular ETL pipeline for Nordtechâ€™s eâ€‘commerce dataset.  
This project extracts, cleans, transforms, enriches, and loads data into a SQLite database, applies BERT sentiment analysis, and generates businessâ€‘ready KPIs and visualizations.

This README is written as a **developerâ€‘style tutorial** so anyone can run and understand the pipeline.

---

## ğŸ“ Project Structure

```
NORDTECH_ETL_PROJECT/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ nordtech_data.csv
â”‚   â”‚   â””â”€â”€ nordtech_validation.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ nordtech_cleaned.csv
â”‚
â”œâ”€â”€ database/
â”‚   â””â”€â”€ nordtech.db
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_pipeline_dev.ipynb
â”‚   â””â”€â”€ 03_kpi_analysis.ipynb
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â””â”€â”€ reflection.pdf
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ sentiment.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1. Create and activate a virtual environment

```bash
python -m venv venv
venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Run the ETL Pipeline

Execute the full ETL process with:

```bash
python run_pipeline.py
```

This script performs:

```
[1] Extract   â†’ Load raw CSV files
[2] Transform â†’ Clean, normalize, engineer features
[3] Sentiment â†’ Apply BERT model to review text
[4] Load      â†’ Save cleaned CSV + write to SQLite
```

---

## ğŸ§© ETL Modules Overview

### ğŸ”§ `src/config.py`
Centralized configuration for all file paths:

```python
RAW_MAIN = "data/raw/nordtech_data.csv"
RAW_VAL = "data/raw/nordtech_validation.csv"
CLEANED = "data/processed/nordtech_cleaned.csv"
DB_PATH = "database/nordtech.db"
TABLE_NAME = "clean_orders"
```

---

### ğŸ“¥ `src/extract.py`
Loads raw datasets:

```python
df_raw = load_main_data()
```

---

### ğŸ§¼ `src/transform.py`
Applies all cleaning steps:

- Standardizes column names  
- Cleans IDs  
- Parses mixed date formats  
- Fixes reversed dates  
- Normalizes regions, payment methods, customer types  
- Cleans Swedish number words  
- Cleans prices  
- Cleans ratings  
- Cleans review text  
- Removes duplicates  

Usage:

```python
df_clean = transform_data(df_raw)
```

---

### ğŸ’¬ `src/sentiment.py`
Adds sentiment using multilingual BERT:

```python
df_clean = add_sentiment_column(df_clean, text_column="recension_text")
```

Sentiment categories:

```
positive
neutral
negative
```

---

### ğŸ“¤ `src/load.py`
Saves outputs using paths from `config.py`:

```python
save_cleaned_csv(df_clean)
load_to_sqlite(df_clean)
```

---

## ğŸ—„ï¸ Database Output

The cleaned dataset is stored in:

```
database/nordtech.db
```

Table name:

```
clean_orders
```

Example query:

```sql
SELECT region, SUM(total_price)
FROM clean_orders
GROUP BY region;
```

---

## ğŸ“Š KPI Analysis

All KPI visualizations are created in:

```
notebooks/03_kpi_analysis.ipynb
```

Figures include:

- Revenue by month  
- Revenue by category  
- Revenue by region  
- Top 10 best sellers  
- Delivery time distribution  
- Rating distribution  
- Sentiment distribution  
- Orders per customer  

---

## ğŸ“˜ Documentation

- **Data Dictionary** â†’ `reports/data_dictionary.md`  
- **Reflection** â†’ `reports/reflection.pdf`  

---

## ğŸ› ï¸ Tech Stack

```
Python
pandas
numpy
matplotlib / seaborn
transformers (BERT)
SQLite
Jupyter Notebook
VS Code
```

---

## ğŸ¯ Project Goals

```
âœ” Build a professional ETL pipeline
âœ” Clean and standardize messy real-world data
âœ” Apply NLP sentiment analysis
âœ” Generate business-ready KPIs
âœ” Produce clear visualizations for presentation
```

---

## ğŸ‘©â€ğŸ’» Author

**Zinah**  
Data Manager Student  
Stockholm, Sweden
